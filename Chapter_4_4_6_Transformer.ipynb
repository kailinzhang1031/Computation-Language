{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1434,  0.9633, -1.5751,  0.7553],\n",
      "         [-0.9502,  1.3529, -0.9713,  0.5686],\n",
      "         [-1.2162, -0.7572,  1.0029,  0.9705]],\n",
      "\n",
      "        [[-0.7565, -0.8175,  1.6590, -0.0849],\n",
      "         [-0.7815, -0.7310,  1.6812, -0.1687],\n",
      "         [ 0.0876, -1.2740,  1.5057, -0.3193]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "from torch import  nn\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=4,nhead=2)\n",
    "src = torch.rand(2,3,4)\n",
    "out = encoder_layer(src)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (layers): ModuleList(\n",
      "    (0): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (2): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (3): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (4): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=4, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=4, bias=True)\n",
      "      (norm1): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((4,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "tensor([[[-0.5728,  0.6944, -1.3222,  1.2006],\n",
      "         [-0.8644,  0.8568, -1.1184,  1.1260],\n",
      "         [-0.4896, -1.3083,  1.3639,  0.4340]],\n",
      "\n",
      "        [[-1.3229, -0.4483,  1.3756,  0.3957],\n",
      "         [-1.3500, -0.2591,  1.4432,  0.1658],\n",
      "         [ 0.4292, -1.4987,  1.2410, -0.1715]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stack all Transformer blocks to build a full nn.TransformerEncoder\n",
    "\"\"\"\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer,num_layers=6)\n",
    "out = transformer_encoder(src)\n",
    "print(transformer_encoder)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2328, -1.4022,  1.3897, -0.2203],\n",
      "         [ 0.3435, -1.3405,  1.3896, -0.3926],\n",
      "         [-1.2061,  1.4346, -0.5944,  0.3659]],\n",
      "\n",
      "        [[ 0.3697, -1.2108,  1.4314, -0.5903],\n",
      "         [ 0.1716, -1.3069,  1.4671, -0.3318],\n",
      "         [-1.2159,  1.3384, -0.6624,  0.5399]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Decoder Modules\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Stack all TransformerDecoderLayer blocks to build a nn.TransformerDecoder\n",
    "\"\"\"\n",
    "memory = transformer_encoder(src)\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=4, nhead=2)\n",
    "transformer_encoder = nn.TransformerDecoder(decoder_layer,num_layers=6)\n",
    "out_part = torch.rand(2,3,4)\n",
    "out = transformer_encoder(out_part,memory)\n",
    "print(out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6f055472",
   "language": "python",
   "display_name": "PyCharm (NLP_BasedOnPretrainedModel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}