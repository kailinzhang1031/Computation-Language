{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: \n",
      "linear1.weight tensor([[ 1.1482,  1.1440],\n",
      "        [ 0.1625,  0.0901],\n",
      "        [-0.3316,  0.0201],\n",
      "        [ 0.8545, -0.7903],\n",
      "        [ 0.2563,  0.9274]])\n",
      "linear1.bias tensor([-1.1406e+00, -2.5449e-01,  2.6309e-01,  1.4399e-06,  3.9960e-01])\n",
      "linear2.weight tensor([[ 0.9841, -0.3626,  0.0552, -0.5975, -0.5715],\n",
      "        [-1.5872, -0.2109, -0.0466,  0.6705,  0.2745]])\n",
      "linear2.bias tensor([0.5184, 0.1918])\n",
      "Predicted results:  tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_class):\n",
    "        super(MLP,self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim,hidden_dim)\n",
    "        self.activate = F.relu\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "    def forward(self,inputs):\n",
    "        hidden = self.linear1(inputs)\n",
    "        activation = self.activate(hidden)\n",
    "        outputs = self.linear2(activation)\n",
    "        log_probs = F.log_softmax(outputs,dim=1)\n",
    "        return log_probs\n",
    "x_train = torch.tensor([[0.0, 0.0],[0.0, 1.0],[1.0,0.0],[1.0,1.0]])\n",
    "y_train = torch.tensor([0,1,1,0])\n",
    "\n",
    "model = MLP(input_dim=2,hidden_dim=5,num_class =2)\n",
    "# num_class: 2 classes\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.05)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_train)\n",
    "    # print(y_pred)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    # zero gradient before call backward\n",
    "    # for every lop, gradient will accumulate\n",
    "    loss.backward()\n",
    "    # calculate gradient in parameters by backward casting\n",
    "    optimizer.step()\n",
    "    # update parameters,\n",
    "    # different optimizers, different methods to update,\n",
    "    # the same method to call\n",
    "\n",
    "print(\"Parameters: \")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "\n",
    "y_pred = model(x_train)\n",
    "print(\"Predicted results: \", y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "linear1.weight tensor([[-1.3549,  1.3495],\n",
      "        [ 0.5163, -0.6616],\n",
      "        [ 0.5987, -0.3991],\n",
      "        [-0.3902, -0.3961],\n",
      "        [-0.1047,  0.8906]])\n",
      "linear1.bias tensor([-0.0034, -0.5163,  0.4516, -0.5412,  0.1068])\n",
      "linear2.weight tensor([[-1.2619,  0.0701, -0.2902,  0.0956,  0.6440],\n",
      "        [ 1.3028,  0.0567,  0.3534,  0.1888, -0.2189]])\n",
      "linear2.bias tensor([ 0.2217, -0.3938])\n",
      "Predicted results: tensor([0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_class):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activate = F.relu\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        hidden = self.linear1(inputs)\n",
    "        activation = self.activate(hidden)\n",
    "        outputs = self.linear2(activation)\n",
    "        # 获得每个输入属于某一类别的概率（Softmax），然后再取对数\n",
    "        # 取对数的目的是避免计算Softmax时可能产生的数值溢出问题\n",
    "        log_probs = F.log_softmax(outputs, dim=1)\n",
    "        return log_probs\n",
    "# 异或问题的4个输入\n",
    "x_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "# 每个输入对应的输出类别\n",
    "y_train = torch.tensor([0, 1, 1, 0])\n",
    "\n",
    "# 创建多层感知器模型，输入层大小为2，隐含层大小为5，输出层大小为2（即有两个类别）\n",
    "model = MLP(input_dim=2, hidden_dim=5, num_class=2)\n",
    "\n",
    "criterion = nn.NLLLoss() # 当使用log_softmax输出时，需要调用负对数似然损失（Negative Log Likelihood，NLL）\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05) # 使用梯度下降参数优化方法，学习率设置为0.05\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x_train) # 调用模型，预测输出结果\n",
    "    loss = criterion(y_pred, y_train) # 通过对比预测结果与正确的结果，计算损失\n",
    "    optimizer.zero_grad() # 在调用反向传播算法之前，将优化器的梯度值置为零，否则每次循环的梯度将进行累加\n",
    "    loss.backward() # 通过反向传播计算参数的梯度\n",
    "    optimizer.step() # 在优化器中更新参数，不同优化器更新的方法不同，但是调用方式相同\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print (name, param.data)\n",
    "\n",
    "y_pred = model(x_train)\n",
    "print(\"Predicted results:\", y_pred.argmax(axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-6f055472",
   "language": "python",
   "display_name": "PyCharm (NLP_BasedOnPretrainedModel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}